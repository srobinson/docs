# Memory Proposal

## The Irony

This proposal was generated by an LLM in conversation. It contains valuable thinking. It will be lost when the context window fills up — which is the exact problem it describes. We are the proof of the problem.

## The Observation

Every time a Nancy worker starts an iteration, it rediscovers the same things. Reads the same files, greps for the same patterns, builds the same mental model — then burns context doing it. Next iteration? Gone. Starts over.

fmm helps with the code navigation part — here's a static map, don't explore blindly. But the deeper problem: **the agent has no memory of what it already learned.**

Humans don't re-read every file every morning. They carry a working model — "auth is in these 3 files, the config system is janky, don't touch the legacy endpoint." That's accumulated knowledge from exploration. Agents have none of this.

## What Nancy Logs Already Capture

Nancy's formatted logs contain everything implicitly:
- Every `Read`, `Grep`, `Bash` command and its output
- The agent's reasoning about what it found
- Dead ends and failed approaches
- Environment quirks discovered during execution

But it's in narrative form — a 500-line stream-of-consciousness no future agent can efficiently consume.

## Proposal: Structured Memory Distilled From Logs

Distill Nancy logs into a structured memory file that the next agent session loads at start.

### Three Layers

**Layer 1: Search traces.** Agent searches for something, finds it. That search-result mapping gets persisted. Next agent doesn't search — it looks up the trace. "Where is the auth middleware?" → already answered, here's the file, here's the line.

**Layer 2: Discovery artifacts.** When the agent reads a file and forms an understanding — "this file handles webhook routing, it depends on config.ts and media-stream.ts, the key function is handleIncoming()" — that interpretation gets persisted. Not the file contents. The understanding. This is what fmm does statically. Search traces capture the dynamic, task-specific understanding.

**Layer 3: Discardable vs durable memory.** Some knowledge is task-specific — "I tried approach X, it failed because Y." Valuable now, worthless after. Other knowledge is durable — "the test suite takes 45 seconds, run with `just check`." The agent or orchestrator should tag which is which.

### Example Memory File

```yaml
# .nancy/memory/ALP-76.yaml
discoveries:
  - query: "where are errors defined"
    result: src/errors/index.ts
    exports: [MdContextError, ParseError, IoError, IndexError]
    pattern: Data.TaggedError from Effect

  - query: "how does CLI error handling work"
    result: src/cli/error-handler.ts
    notes: "formatError maps domain errors to user-friendly output"

environment:
  build_cmd: "just check"
  test_cmd: "pnpm test"
  lint_issue: "biome worktree needs root:true in biome.json"

dead_ends:
  - tried: "$root in biome.json"
    failed: "key is 'root' not '$root'"
```

50 lines of YAML vs re-reading 20 source files. The dead_ends section is gold — negative knowledge that prevents repeating mistakes.

### Who Writes the Memory?

Three options, in order of preference:

1. **Orchestrator extracts** (preferred) — The orchestrator reads the worker's log after each iteration and distills structured memory. Fits Nancy's architecture — orchestrator already watches the worker. Keeps cost out of the worker's context budget.

2. **Post-iteration distillation** — A cheap model reads the log and extracts structured memory. Automated, might miss nuance.

3. **Agent self-reports** — Worker writes its own memory before session ends. It knows what it learned, but burns tokens to do it while running out of context.

## Open Questions

- What's the right format? YAML is readable but maybe too flat for complex discoveries.
- How does memory accumulate across iterations without growing unbounded?
- Should memory be per-task, per-project, or both?
- How does this relate to fmm? fmm is static code metadata. Memory is dynamic task knowledge. They're complementary — fmm is the map, memory is the journal.
- Can we validate this by replaying a Nancy task with pre-loaded memory vs without?

## Critical Reframe: Navigation, Not Memory

The initial proposal (above) is flawed. Loading memory into the prompt IS consuming the context window. Even 50 lines of YAML is tokens the agent can't use for the actual task.

**The constraint:** For any given task, the LLM needs the fullest amount of free context to complete it. Navigation should consume almost zero tokens.

**You cannot feed the LLM memory. The LLM must be able to navigate TO memory on demand.**

### The Subagent Model

Claude can spawn subagents. This changes the economics:

```
Parent agent (precious context — reserved for actual task work)
  ├── subagent: "find where auth middleware is defined" → "src/middleware/auth.ts:42"
  ├── subagent: "what does the config schema look like" → "3 fields: host, port, db"
  └── subagent: "run tests, tell me what failed" → "2 failures in auth.test.ts"
```

- Parent burns ~20 tokens per lookup (the question + the summary answer)
- Subagent burns 2000 tokens exploring, then its context is discarded
- Parent's window stays clean for implementation

Two paths for subagent results:
1. **Return a summary** — parent gets a black-box answer, never sees the raw data
2. **Generate more markdown** — the exploration is persisted as a doc, queryable later

Both are valid. Option 1 protects parent context. Option 2 builds a knowledge base (which loops back to the mdcontext problem — searchable markdown).

### How fmm + Subagents Compound

- Without fmm: subagent reads 10 files to find the answer → burns 5000 tokens
- With fmm: subagent reads 3 sidecars, opens 1 file → burns 800 tokens
- The subagent is disposable, but cheaper subagents = more lookups within budget

### Nancy Logs as Training Data

Nancy logs contain every search, every read, every dead end. But NOT as context to load — as data to analyze:

- Which lookups do agents commonly need at task start?
- Which explorations are repeated across iterations?
- Where does the agent waste the most tokens on navigation vs actual work?

This analysis tells us what to pre-compute (fmm sidecars, project-level indexes) vs what to look up on demand (task-specific state).

### The Architecture

```
Static layer (pre-computed, zero runtime cost):
  └── fmm sidecars: code structure, exports, deps

Dynamic layer (on-demand, subagent cost):
  └── subagent lookups: task-specific questions → summary answers

Analysis layer (offline, improves the other two):
  └── Nancy log analysis: what do agents repeatedly search for?
       → feed back into static layer (pre-compute common lookups)
       → optimize dynamic layer (better subagent prompts)
```

## Second Reframe: Subagents Don't Solve This

The subagent model looks good on paper but it's a black box. Parent spawns subagent, subagent explores, subagent returns summary, parent has to understand the summary. That understanding costs tokens. The summarization is lossy — detail is compressed away. And in a closed loop the parent still needs to hold enough context to make decisions. It's just moving the problem.

**The hard truth: there is no external memory system.** Everything the agent "knows" must be in the context window. Period. Subagents, summaries, memory files — they all end up as tokens in somebody's window.

## The Real Lever: Signal-to-Noise Ratio

The only thing we can control is **how valuable each token in the window is.** Every token should be doing work toward the task. Tokens spent on navigation are waste. Tokens spent rediscovering things are waste. Boilerplate is waste.

This is what fmm actually is — not memory, not navigation. **Compression.** A 500-line source file becomes a 10-line sidecar. The agent gets the same navigational understanding at 2% of the token cost. The remaining 98% is free for actual work.

The question isn't "how do we give the agent memory." It's:

**How do we maximize the signal-to-noise ratio of what's in the window?**

Categories of tokens in a typical agent session:
- **Task tokens** — the actual work (reading code to edit, writing code, reasoning about the problem). HIGH VALUE.
- **Navigation tokens** — figuring out what's where (searching, reading files to understand structure). LOW VALUE. This is what fmm compresses.
- **Rediscovery tokens** — learning things a previous iteration already knew. ZERO VALUE. Pure waste.
- **Boilerplate tokens** — system prompts, tool schemas, repeated instructions. FIXED COST.

Reducing navigation tokens = fmm (proven directionally by ALP-479/480 experiment).
Reducing rediscovery tokens = unsolved. This is the next frontier.
Reducing boilerplate tokens = prompt engineering, but diminishing returns.

## Open Question

How do you eliminate rediscovery without loading memory (which costs tokens)?

Possible angles:
- Make the rediscovery itself cheaper (fmm helps — rediscovering via sidecars costs less than rediscovering via source files)
- Eliminate the need to rediscover by structuring tasks so each iteration is self-contained (Nancy sub-issues partially do this)
- Pre-compute the answers to common lookups and embed them in the codebase itself (fmm sidecars, but richer?)

None of these are proven. This is where experiments need to happen.

## The Thesis

**Make every token in the window as high-value as possible.**

Two sides of the same coin:

### 1. Richer sidecars — the file's control surface

A 10-line sidecar is a compressed summary. Useful but thin. A 50-line sidecar could be a **briefing document** for that file:

- What this file does (summary)
- Exports, imports, dependencies (structure)
- Why it exists, what decisions were made (intent)
- What NOT to do — gotchas, constraints, coupling risks (guardrails)
- Version history of significant changes (evolution)
- Instructions for how to modify this file correctly (playbook)

The sidecar becomes the agent's interface to the file. The agent reads the sidecar, understands the file's role AND how to work with it, and only opens the source when it's time to edit. The sidecar is versioned alongside the source — when the file changes, the sidecar evolves.

This isn't memory. It's **pre-computed understanding** embedded in the codebase.

### 2. Agents must write less code per file

This is upstream of everything. If agents produce 500-line files:
- Sidecars are harder to write and maintain
- Future agents burn more tokens understanding them
- Comprehension cost per file goes up
- The whole navigation layer gets heavier

Smaller files = smaller sidecars = cheaper navigation = more context for actual work.

The signal-to-noise thesis applies to code generation itself. Agents need instructions (possibly in the sidecars themselves) to keep files small, focused, and single-purpose. The sidecar for a 50-line file might be 10 lines. The sidecar for a 500-line file might be 80 lines and still lose information.

### The feedback loop

```
Smaller files
  → cheaper sidecars
    → cheaper navigation
      → more free context for task work
        → better code output
          → smaller, better-structured files
```

This is the virtuous cycle. fmm isn't just a navigation tool — it's a forcing function for better code architecture, because the economics of the context window reward small, well-documented, single-purpose files.

## Real Use Case: openclaw/openclaw#7027

Telegram channel_post support. ~3000 file repo. Fresh clone. Zero local knowledge.

### What the agent actually does (no fmm)

```
1. Clone repo. 3000 files land.
2. Read GitHub issue → understand the task.
3. THE SCRAMBLE:
   grep "telegram" across 3000 files           → 200+ matches
   grep "channel_post"                          → 0 matches (it's new)
   grep "message" in telegram dir               → too many
   Read package.json                            → find grammy dep
   Read 5-10 telegram plugin files              → 500-2000 lines each
   Read session routing code                    → 3-4 more files
   Read config schema                           → another file
   Read group handler as template               → another file
   ─────────────────────────────────────────────
   ~15-20 file reads before writing a single line.
   5000-10000 tokens on navigation alone.
   Plus reasoning about each file doubles it.
```

### What the agent does (with fmm)

```
1. Clone repo. 3000 files land.
2. fmm generate → sidecars created.
3. Read GitHub issue → understand the task.
4. fmm search "telegram plugin" → 3 files.
   Read 5-10 sidecars (50 lines each = 500 tokens).
   Know exactly which 2-3 source files to open.
   Get to work.
```

### The two unsolved problems

**Problem 1: fmm doesn't exist yet for a fresh repo.**
Someone/something has to generate it. This is a one-time cost per repo but it's real. Could be a setup step (`fmm generate`) or could be a background process that runs alongside the agent.

**Problem 2: fmm search needs to be GOOD.**
`fmm search "telegram session routing"` needs to return the right files, not just keyword-match against sidecar text. The search tooling IS the product. Bad search = agent falls back to grep = back to the scramble.

### The tail-process idea

A separate LLM process tails the agent's logs in real-time and:
- Watches what the agent searches for
- Watches what files the agent reads
- Maintains/enriches fmm sidecars based on what the agent discovers
- Optimizes the fmm index based on actual usage patterns

**Concern:** This is another LLM with its own context window. More context rot. More token spend. The value has to clearly exceed the cost.

**Counter-argument:** This process doesn't need to be an LLM. The log-tailing and sidecar enrichment could be deterministic — pattern-match on Read/Grep tool calls, track which files were accessed, update sidecar "last accessed" / "commonly queried" metadata. Save LLM for the parts that actually need reasoning (e.g., updating a sidecar's "what this file does" summary after the agent modified it).

### What needs to be built

Two things, not one:

1. **fmm itself** — the sidecar generator and format. This exists. Needs richer sidecars (the 50-line briefing doc, not just 10-line metadata).

2. **fmm search tooling** — the thing that makes sidecars queryable. This is the make-or-break. Options:
   - Simple: keyword search across sidecar YAML fields
   - Better: semantic search over sidecar content (mdcontext-style)
   - Best: structured queries — "find all files that export a function matching `handle*` in the telegram plugin directory that depend on the session module"

The search tooling determines whether fmm is "slightly better than grep" or "fundamentally changes how agents navigate code."

## Experiment Results: The Numbers Are In

### ALP-487: Nancy Log Analyzer

Built `src/analyze/` — Python module that parses raw NDJSON Nancy logs, classifies every event (navigation/fmm/task-work/boilerplate), and generates comparison tables. 90 tests. PR: https://github.com/srobinson/nancy/pull/1

### ALP-479 (fmm) vs ALP-480 (no fmm) — OpenClaw #5606

```
                           Control (no fmm)    Treatment (fmm)
Reads before first edit:   25                  40
Navigation tokens:         ~30,292             ~15,884
Navigation % of total:     52%                 41%
Time to first edit:        iter1 @ 48%         iter1 @ 13%
Sidecar lookups:           n/a                 0/55
Total tokens:              ~58,310             ~39,111
```

### What the data proves

1. **Navigation is the dominant cost.** 52% (control) and 41% (treatment) of all tokens spent on navigation. Not task work. Not boilerplate. Navigation. The thesis is directionally correct — this is where the wins are.

2. **fmm condition was 33% more token-efficient** despite running more iterations (4 vs 3). Reached first edit at 13% into iter1 vs 48% for control. Oriented faster, spent less.

3. **The agent completely ignored sidecars.** 0 out of 55 navigation calls used fmm tools or read `.fmm` files. The sidecars were available. The instructions were in CLAUDE.md. The agent grepped and read raw source files anyway.

### What this means for the thesis

The signal-to-noise thesis holds: navigation dominates the window, and compressing it would free massive headroom. But **the tooling alone isn't enough.** The agent must be *forced* to use sidecars, not merely offered them.

This is the instruction compliance problem. CLAUDE.md says "read sidecars first." The agent doesn't. The instruction gets diluted by the task prompt, tool schemas, and the agent's own training priors (which favor grep/read patterns it's seen in training data).

### The next wall: instruction compliance

Three angles to attack:

1. **Task-prompt level instructions** — not just CLAUDE.md, but embedded in every task prompt Nancy generates. "Before reading any source file, check if a .fmm sidecar exists."

2. **Tool restriction** — remove raw Read/Grep from the tool set and replace with fmm-aware equivalents that check sidecars first, falling back to source only when needed.

3. **Sidecar-as-gateway** — make the sidecar contain enough information that the agent doesn't need the source file for navigation at all. Only open source to edit. The sidecar IS the file for reading purposes.

None of these are proven. But we now have the harness to measure them.
