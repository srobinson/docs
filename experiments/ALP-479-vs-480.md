# Experiment: ALP-479 (fmm) vs ALP-480 (no fmm)

**Task:** OpenClaw #5606 — voice call extension
**Repo:** openclaw/openclaw
**Date:** 2026-02-02
**Analyzer:** `nancy src/analyze`

## Comparison Table

```
                           Control (no fmm)    Treatment (fmm)
                           ──────────────────  ─────────────────
Reads before first edit:   25                  40
Greps before first edit:   2                   0
Unique files discovered:   15                  24
Navigation tokens:         ~30,292             ~15,884
Navigation % of total:     52%                 41%
Time to first edit:        iter1 @ 48%         iter1 @ 13%
Sidecar lookups:           n/a                 0/55
Task complete:             yes                 yes
Total iterations:          3                   4
Total tokens:              ~58,310             ~39,111
```

## Per-Iteration Detail

### Control (ALP-480, no fmm)

| Iteration | Nav Calls | Nav Tokens | Total Tokens | First Edit |
|-----------|-----------|------------|--------------|------------|
| iter1 | 20 | 24,816 | 51,187 | seq 95 |
| iter2 | 18 | 2,763 | 2,763 | no edits |
| iter2-review | 18 | 2,713 | 4,360 | seq 85 |

### Treatment (ALP-479, fmm available)

| Iteration | Nav Calls | Nav Tokens | Total Tokens | First Edit |
|-----------|-----------|------------|--------------|------------|
| iter1 | 39 | 3,040 | 24,134 | seq 151 |
| iter2 | 9 | 1,391 | 1,391 | no edits |
| iter3 | 8 | 1,286 | 1,286 | no edits |
| iter3-review | 13 | 10,167 | 12,300 | seq 64 |

## Analysis

### Key findings

1. **Treatment used 33% fewer total tokens** (39k vs 58k). The fmm condition was more token-efficient overall, despite having more iterations.

2. **Treatment reached first edit much earlier** within iter1 (13% into the session vs 48% for control). This suggests the agent oriented faster even though it made more navigation calls (39 vs 20).

3. **Zero sidecar usage.** The treatment agent had fmm sidecars available but never used `mcp__mcp-files__read_symbol` or read any `.fmm` files. All 55 navigation lookups were raw reads/greps. This confirms the instruction compliance problem described in `EXPERIMENT_HARNESS.md` — the agent ignores sidecar instructions under task pressure.

4. **More nav calls in treatment, but fewer nav tokens.** Treatment made 69 total nav calls across all iterations vs 56 for control, yet spent only ~16k nav tokens vs ~30k. This suggests treatment's nav calls were lighter-weight (reading smaller files or getting cache hits).

5. **Navigation dominated both conditions.** Control spent 52% of tokens on navigation, treatment 41%. In both cases, navigation is the largest cost center. This validates the thesis that reducing navigation overhead is the key lever.

### Caveats

- Single experiment pair (n=1). Cannot draw statistical conclusions.
- The tasks may not be identical in difficulty — control and treatment worked on the same GitHub issue but in different repo states.
- "No edits" iterations (iter2 in control, iter2+3 in treatment) appear to be sessions that ran out of context or were interrupted before productive work.
- Token counts are `input_tokens + output_tokens` only; cache tokens (creation + read) are tracked separately and not included in the headline numbers. Cache tokens represent the prompt context sent to the model.

### Next steps

- **Fix sidecar instruction compliance.** The 0/55 sidecar usage is the most actionable finding. The agent needs stronger nudges: fmm MCP as primary search tool, sidecar-first instructions in the task prompt, or tool availability restrictions.
- **Run experiment 2** on a different issue to see if the token efficiency difference holds.
- **Investigate cache token patterns** — the large cache_creation and cache_read numbers may reveal interesting patterns about context reuse between messages.
